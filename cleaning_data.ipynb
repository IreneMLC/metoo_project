{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "\n",
    "csv_filename = 'C:/Users/Robin/Dropbox/Travaux_&_Rapports_Stages/UbicomLab-GATech/MeToo/code/Data_tweets/tweetsbalancetonporc.csv'\n",
    "json_filename='C:/Users/Robin/Dropbox/Travaux_&_Rapports_Stages/UbicomLab-GATech/MeToo/code/Data_tweets/tweetsbalancetonporc-13_10-11-8.json'\n",
    "json_filename2='C:/Users/Robin/Dropbox/Travaux_&_Rapports_Stages/UbicomLab-GATech/MeToo/code/Data_tweets/tweetsbalancetonporc-13_10-11-8 - Copie.json' \n",
    "json_metoo='C:/Users/Robin/Dropbox/Travaux_&_Rapports_Stages/UbicomLab-GATech/MeToo/code/data-metoo/metoo.json'\n",
    "df = pd.read_json(json_filename, orient=\"columns\")\n",
    "dfc=pd.read_json(json_filename2, orient=\"columns\")\n",
    "df_metoo=pd.read_json(json_metoo,orient='colums')\n",
    "df_metoo_c=pd.read_json(json_metoo,orient='colums')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il y a  fullname     5478\n",
      "id           5478\n",
      "likes        5478\n",
      "replies      5478\n",
      "retweets     5478\n",
      "text         5478\n",
      "timestamp    5478\n",
      "user         5478\n",
      "url          5478\n",
      "url_image    5478\n",
      "hashtag      5478\n",
      "mention      5478\n",
      "langue       5478\n",
      "dtype: int64 qui vont être supprimés\n",
      "76010 76010\n",
      "76010 70532\n"
     ]
    }
   ],
   "source": [
    "step1(df) #extract link, pictures and create a special column\n",
    "step2(df) # extract the # and the @ and creta a special column\n",
    "#step3(df_metoo) # delete punctiation, and figures.\n",
    "sup_lang(df,dfc,\"fr\")# delete tweet which are not in the language define\n",
    "step5(df) # delete the column languange and put everything in lowercase\n",
    "step6(df,'btp_clean_tweet_with_punctiation.json') #save the data under another json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Extract url link from tweet and create a special column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################################################################### STEP 1\n",
    "#Function that enables to delete link in a text\n",
    "remove_url=lambda x: re.sub(find_url, '', x)\n",
    "remove_img=lambda x: re.sub(find_img,' ',x)\n",
    "find_url = re.compile(r'(?:http|ftp|https)://(?:[\\w_-]+(?:(?:\\.[\\w_-]+)+))(?:[\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?')\n",
    "find_img=re.compile(r'(pic.twitter.com/[^\\s]+)')\n",
    "def step1(df):\n",
    "    #create a colum for url link\n",
    "    l=int(len(df['id']))\n",
    "\n",
    "    url_tab=[]\n",
    "    tweet_url=[]\n",
    "    img_tab=[]\n",
    "    for text in df['text']:\n",
    "        url_tab.append(find_url.findall(text))\n",
    "        img_tab.append(find_img.findall(text))\n",
    "    #print(url_tab)\n",
    "    tweet_url=[]*len(url_tab)\n",
    "    tweet_img=[]*len(img_tab)       \n",
    "    for i in range(len(url_tab)):\n",
    "        if len(url_tab[i])==0 or url_tab[i][0]==0:\n",
    "            url_tab[i]=[\"\"]\n",
    "        tweet_url.append(url_tab[i][0])\n",
    "        if len(img_tab[i])==0 or img_tab[i][0]==0:\n",
    "            img_tab[i]=[\"\"]\n",
    "        tweet_img.append(img_tab[i][0])\n",
    "     \n",
    "    df['url']=tweet_url#add a column\n",
    "    df.loc[:,['url']] = df['url'].values\n",
    "\n",
    "    df['url_image']=tweet_img#add a column\n",
    "    df.loc[:,['url_image']] = df['url_image'].values\n",
    "\n",
    "    df['text']=df['text'].map(remove_url) #We apply this function to the serie 'text'\n",
    "    df['text']=df['text'].map(remove_img)\n",
    "#find image link\n",
    "#exemple: tweet numeros: 9516\n",
    "#find_img=re.compile(r'(pic).(twitter).(com)/(([^\\s]+))')\n",
    "#print(find_img.findall(dfc['text'][9516]),dfc['text'][9516])\n",
    "######################################################################################### STEP 2\n",
    "\n",
    "#create a column for #\n",
    "\n",
    "find_hastag = re.compile(r'\\B(\\#[a-zA-Z]+\\b)(?!;)')\n",
    "revome_hastag=lambda x: re.sub(find_hastag, '', x)\n",
    "find_mention = re.compile(r'\\B(\\@[a-zA-Z]+\\b)(?!;)')\n",
    "revome_mention=lambda x: re.sub(find_mention, '', x)\n",
    "\n",
    "def step2(df):\n",
    "    has_tab=[]\n",
    "    men_tab=[]\n",
    "    for text in df['text']:\n",
    "        h=find_hastag.findall(text)\n",
    "        k=find_mention.findall(text)\n",
    "        has_tab.append([h])\n",
    "        men_tab.append([k])\n",
    "    \n",
    "    #print(url_tab)\n",
    "    hashtag_tab=[]*len(has_tab)\n",
    "    mention_tab= []*len(men_tab)\n",
    "    for i in range(len(has_tab)):\n",
    "        if len(has_tab[i])==0 or has_tab[i][0]==0:\n",
    "            has_tab[i]=[\"\"]\n",
    "        hashtag_tab.append(str(has_tab[i][0]))\n",
    "\n",
    "    for i in range(len(men_tab)):\n",
    "        if len(men_tab[i])==0 or men_tab[i][0]==0:\n",
    "            men_tab[i]=[\"\"]\n",
    "        mention_tab.append(str(men_tab[i][0]))\n",
    "    df['hashtag']=hashtag_tab#add a column\n",
    "    df.loc[:,['hashtag']] = df['hashtag'].values\n",
    "    \n",
    "    df['mention']=mention_tab#add a column\n",
    "    df.loc[:,['mention']] = df['mention'].values\n",
    "    #supression des # et des @\n",
    "    df['text']=df['text'].map(revome_hastag)\n",
    "    df['text']=df['text'].map(revome_mention)\n",
    "    \n",
    "    #Convertir les mention et les hashtag en string\n",
    "    #for i in range(len(df['hashtag'])):\n",
    "     #   df['hashtag'][i].to_string\n",
    "    df['hashtag']=df['hashtag'].str.replace('[', '')\n",
    "    df['hashtag']=df['hashtag'].str.replace(']','')\n",
    "    \n",
    "    df['mention']=df['mention'].str.replace('[', '')\n",
    "    df['mention']=df['mention'].str.replace(']','')\n",
    "    \n",
    "    \n",
    "    #print(df['hashtag'][9982],df['text'][9982])\n",
    "    #print(p2.findall(df['text'][1]))\n",
    "    #txt=df['text'][5]\n",
    "    #print(f(txt),txt)\n",
    "\n",
    "######################################################################################### STEP 3\n",
    "\n",
    "#We replace !,.?»« by spaces\n",
    "def step3(df):\n",
    "    g=df['text']#creating a copy for the test\n",
    "    \n",
    "    df['text'] = df['text'].str.replace(',', ' ')\n",
    "    df['text'] = df['text'].str.replace('!', ' ')\n",
    "    df['text'] = df['text'].str.replace('.', ' ')\n",
    "    df['text'] = df['text'].str.replace('?', ' ')\n",
    "    df['text'] = df['text'].str.replace('»', '')\n",
    "    df['text'] = df['text'].str.replace('«', '')\n",
    "    \n",
    "    #We remove all special caracteres : !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "    remove_ponctuation = lambda x: ''.join(ch for ch in x if ch not in set(string.punctuation))\n",
    "    \n",
    "    df['text']=df['text'].map(remove_ponctuation)\n",
    "    \n",
    "    #print(k[5],df['text'][5])\n",
    "    \n",
    "    #remove RT\n",
    "    df['text'] = df['text'].str.replace('RT', ' ')\n",
    "    #remove number\n",
    "    find_numbers = re.compile(r'\\d+')\n",
    "    revome_numbers=lambda x: re.sub(find_numbers, '', x)\n",
    "    df['text']=df['text'].map(revome_numbers)\n",
    "    \n",
    "######################################################################################### STEP SUP_LANG\n",
    "def sup_lang(df,dfc,lg):\n",
    "    #print(nltk.word_tokenize(df['text'][33399]))\n",
    "    detect_lg=lambda x: 1 if detect(x)==lg else 0\n",
    "    nb_mot=lambda x:len(nltk.word_tokenize(x))\n",
    "    l=len(df['text'])\n",
    "    #print(len(df))\n",
    "    langue=[]\n",
    "    k=0\n",
    "    for i in range(l):\n",
    "        \n",
    "        if nb_mot(df['text'].iloc[i])<=2: # If there is less than two word, we consider that the method is ineficient so, we keep it\n",
    "            langue.append('1')\n",
    "            k+=1\n",
    "        else:\n",
    "            try:\n",
    "                langue.append(detect_lg(df['text'].iloc[i]))\n",
    "            except:\n",
    "                langue.append('1')\n",
    "\n",
    "    df['langue']=langue\n",
    "    #print(langue[:100])\n",
    "    print (\"il y a \",df[df['langue']==0].agg('count'),\"qui vont être supprimés\")\n",
    "    print(len(dfc),len(df))\n",
    "    #df=df.drop(df.index[df.text.map(detect_fr)==0]) \n",
    "    #langue=lambda x:len(nltk.word_tokenize(x))\n",
    "    df=df.drop(df.index[df.langue==0])\n",
    "    print(len(dfc),len(df))\n",
    "#df_metoo[df_metoo['langue']==0].agg('count')\n",
    "#df[df['langue']==0].agg('count')\n",
    "#sup_lang(df,dfc,'fr')\n",
    "#sup_lang(df_metoo,df_metoo_c,'en')\n",
    "\n",
    "\n",
    "\n",
    "def step5(df):\n",
    "    #import nltk\n",
    "    #print(nltk.word_tokenize(df['text'][33399]))\n",
    "    #nb_mot=lambda x:len(nltk.word_tokenize(x))\n",
    "    # df['text'] = df['text'].str.replace(',', ' ')\n",
    "    \n",
    "    #Everithing lower cse\n",
    "    df['hashtag'] = df['hashtag'].map(lambda x: x.lower())\n",
    "    df['mention'] = df['mention'].map(lambda x: x.lower())\n",
    "    del df['langue']\n",
    "\n",
    "######################################################################################### STEP 6\n",
    "def step6(df,filename):\n",
    "    df.to_json(filename)\n",
    "    #df2=pd.read_json(filename, orient=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have a look on the first 5 lines\n",
    "print(df.head())\n",
    "#Provide basics stats about this column\n",
    "df.likes.describe()\n",
    "#Select all the Tweets with more than 500 retweets\n",
    "df[df['retweets']>500]\n",
    "#count the number of tweet with this attributes\n",
    "(df[df['retweets']>500]).agg('count')\n",
    "print(df['text'][0])\n",
    "df[df['retweets']>500]['retweets']\n",
    "(df[df['retweets']>500]).agg('count')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
