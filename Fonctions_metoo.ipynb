{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction of metoo project\n",
    "#### this file contain all the main fonction use in my research project on the #metoo campaign and the french equivalent #balancetonporc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pylab\n",
    "import nltk\n",
    "import operator \n",
    "import json\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import  tweepy, sys, locale, threading \n",
    "from time import localtime, strftime, sleep\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "import stop_words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams \n",
    "from nltk import trigrams\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import wordcloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from labMTsimple.storyLab import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## behavior function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_picture(df):\n",
    "    df_url_i=df[df['url_image'].apply(lambda x: '/' in x)]\n",
    "    per=round(len(df_url_i)/len(df)*100,3)\n",
    "    print(per,'% of the tweet have a picture')\n",
    "    return(df_url_i)\n",
    "#################\n",
    "def has_url(df):\n",
    "    df_url=df[df['url'].apply(lambda x: '/' in x)]\n",
    "    per=round(len(df_url)/len(df)*100,3)\n",
    "    print(per,'% of the tweet have a url')\n",
    "    return(df_url)\n",
    "#################\n",
    "def has_mention(df):\n",
    "    df_men=df[df['mention'].apply(lambda x: '@' in x)]\n",
    "    per=round(len(df_men)/len(df)*100,3)\n",
    "    print(per,'% of the tweet have a mention')\n",
    "    return(df_men)\n",
    "#################\n",
    "def mean_nb_word(df):\n",
    "    s=(df['nb_mots']).sum()\n",
    "    m=s/len(df)\n",
    "    print('the average number of word =',round(m,3))\n",
    "    return(m)\n",
    "#############\n",
    "def behavior(df):\n",
    "    i1=mean_nb_word(df)\n",
    "    i2=has_mention(df)\n",
    "    i3=has_url(df)\n",
    "    i4=has_picture(df)\n",
    "    return([i1,i2,i3,i4])\n",
    "########\n",
    "\n",
    "def distribution_word(df):\n",
    "    df_mot=df.groupby(df['nb_mots']).count()\n",
    "    df_mot.to_excel('word_dist_en.xlsx')\n",
    "    return(df_mot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## descriptive statistic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_by_user(tweet_df) : #give back a data frame with the number of tweet for each user /!\\ very heavy !!\n",
    "    DU=[]\n",
    "    U=[]\n",
    "    I=[]\n",
    "    nb_user=len(tweet_df['fullname'].value_counts())\n",
    "    df=tweet_df['fullname'].value_counts().sort_values(ascending=False)\n",
    "    for i in range(nb_user):\n",
    "        DU.append(df.iloc[i])\n",
    "        U.append(df.index[i])\n",
    "        I.append(i)\n",
    "    d = {'fullname': U, 'nb_tweet': DU}\n",
    "    df_user = pd.DataFrame(data=d)\n",
    "    ####\n",
    "    fig1 = plt.figure(figsize = (30, 10))\n",
    "    ax = fig1.add_subplot(111)\n",
    "    \n",
    "    x = I\n",
    "    height = DU\n",
    "    width = 0.5\n",
    "    plt.tight_layout() #ajustement des etiquttes\n",
    "    \n",
    "     #fisrt graph\n",
    "    plt.bar(x, height, width, color='b')\n",
    "    \n",
    "    ax.set_xticklabels(U)\n",
    "    plt.ylim(0,300)\n",
    "    plt.xlim(0,100)\n",
    "    plt.title('user repartition')\n",
    "    plt.show()\n",
    "    return(df_user)\n",
    "\n",
    "def basic_statistic(tweet_df):\n",
    "    \n",
    "    # Number of tweets\n",
    "    nb_tweet=len(tweet_df.index)\n",
    "    \n",
    "    print('the campaign is composed of',nb_tweet,'tweets')\n",
    "    print('\\n')\n",
    "    \n",
    "    # Number of users involved in the campaign\n",
    "    nb_users=len(tweet_df['fullname'].value_counts())\n",
    "    \n",
    "    print('This Tweets are coming from ',nb_users,'different users')\n",
    "    print('\\n')\n",
    "    # top users\n",
    "    print('The top-10 users are :','\\n')\n",
    "    for i in range(10):\n",
    "        print(tweet_df['fullname'].iloc[i], ' with ', tweet_df['fullname'].value_counts().iloc[i],'tweets')\n",
    "    print('\\n')\n",
    "    # tweet:\n",
    "    mean_nb_tweet=round(tweet_df['fullname'].value_counts().mean(),2)\n",
    "    std_nb_tweet=round(tweet_df['fullname'].value_counts().std(),2)\n",
    "    med_nb_tweet=round(tweet_df['fullname'].value_counts().median(),2)\n",
    "    \n",
    "    print('The average number of tweet by user is :',mean_nb_tweet)\n",
    "    print('The standard deviation of the number of tweets by user is :',std_nb_tweet)\n",
    "    print('The  median of the number of tweets by user is :',)\n",
    "    print('\\n')\n",
    "    \n",
    "    # Retweets\n",
    "    nb_retweet=sum(tweet_df['retweets'])/nb_tweet*100\n",
    "    nb_tweet_retweeted=len(tweet_df[tweet_df['retweets']>1]) /nb_tweet*100\n",
    "    med_retweet=round(tweet_df['retweets'].mean(),2)\n",
    "    \n",
    "    print('The number of retweets involved in this campaign is',nb_retweet)\n",
    "    print('Among the',nb_tweet,'tweets involved in the campaign,',nb_tweet_retweeted,'were retweeted at least one time')\n",
    "    print('Each tweets was retweeted on average(Mean) of:',  med_retweet,'times', '\\n')\n",
    "    \n",
    "    print('\\n')\n",
    "    tweet_df = tweet_df.sort_values(by='retweets', ascending=False)\n",
    "    tweet_df = tweet_df.reset_index(drop=True)\n",
    "   \n",
    "    print('Top 5 Retweeted tweets:')\n",
    "    print('------------------')\n",
    "    for i in range(5):\n",
    "        print(tweet_df['text'].iloc[i], '-', tweet_df['retweets'].iloc[i],'retweets')\n",
    "    print('\\n')\n",
    "    # Likes\n",
    "    nb_like=sum(tweet_df['likes'])/nb_tweet*100\n",
    "    nb_tweet_liked=len(tweet_df[tweet_df['likes']>1])/nb_tweet*100\n",
    "    mean_like=round(tweet_df['likes'].mean(),2)\n",
    "    print('The number of likes involved in this campaign is',nb_like)\n",
    "    print('Among the',nb_tweet,'tweets involved in the campaign,',nb_tweet_liked,'were liked at least one time')\n",
    "    print('\\n')\n",
    "    tweet_df = tweet_df.sort_values(by='likes', ascending=False)\n",
    "    tweet_df = tweet_df.reset_index(drop=True)\n",
    "    print('Each tweets was liked on average(Mean) of:',mean_like ,'times', '\\n')\n",
    "    print('Top 5 liked tweets:')\n",
    "    print('-------------------')\n",
    "    for i in range(5):\n",
    "        print(tweet_df['text'].iloc[i], '-', tweet_df['likes'].iloc[i],'likes')\n",
    "    print('\\n')\n",
    "\n",
    "    I=['nb_tweet','nb_users','mean_nb_tweet','std_nb_tweet','med_nb_tweet','nb_retweet','nb_tweet_retweeted','nb_like','nb_tweet_liked','mean_like']\n",
    "    BS=[nb_tweet,nb_users,mean_nb_tweet,std_nb_tweet,med_nb_tweet,nb_retweet,nb_tweet_retweeted,nb_like,nb_tweet_liked,mean_like]\n",
    "    \n",
    "    return(BS,I)\n",
    "\n",
    "def compare_stat(S1,name1,S2,name2,I):\n",
    "    d = {name1 : S1, name2 : S2}\n",
    "    df= pd.DataFrame(index=I,data=d)\n",
    "    df.to_excel('compare_stat.xlsx')\n",
    "    return(df)\n",
    "\n",
    "####################### plot time distribution\n",
    "def time_distribution(tweet_df):\n",
    "    pd.to_datetime(tweet_df['timestamp'])\n",
    "    #Create a graph representing the activity of the campaign by day:\n",
    "    df_D=tweet_df.set_index('timestamp').groupby(pd.TimeGrouper('D')).count().dropna()\n",
    "    df_H=tweet_df.set_index('timestamp').groupby(pd.TimeGrouper('H')).count().dropna()\n",
    "    \n",
    "    times = pd.to_datetime(tweet_df['timestamp'])\n",
    "    #print(times.dt.hour)\n",
    "    df_H24=tweet_df.reset_index(drop=True).groupby([times.dt.hour]).count().dropna()\n",
    "    ################ save and print in excel\n",
    "    writer = pd.ExcelWriter('time_distribution_fr.xlsx')\n",
    "    df_D.to_excel(writer,'day_distribution')\n",
    "    df_H.to_excel(writer,'hour_distribution')\n",
    "    df_H24.to_excel(writer,'h24_distribution')\n",
    "    writer.save()\n",
    "    ############## print in python\n",
    "    \n",
    "    dataD=df_D['text']\n",
    "    dataH=df_H['text']\n",
    "    dataH24=df_H24['text']\n",
    "    \n",
    "    NTD=[]\n",
    "    NTH=[]\n",
    "    NTH24=[]\n",
    "    I=[]\n",
    "    \n",
    "    SNTH=[]\n",
    "\n",
    "    for i in range(len(dataD)):\n",
    "        NTD.append(dataD[i])\n",
    "    for i in range(len(dataH)):\n",
    "        NTH.append(dataH[i])\n",
    "    \n",
    "    for i in range(len(dataH24)):\n",
    "        NTH24.append(dataH24.iloc[i])\n",
    "        I.append(i)\n",
    "    \n",
    "    print(NTH24[:10]) \n",
    "    \n",
    "    SNTH=[NTH[0]]\n",
    "    for i in range(len(NTH)-1):\n",
    "        SNTH.append(SNTH[i]+NTH[i+1])\n",
    "    \n",
    "#print(TD,data,NTD)\n",
    "#print(tweet_df['timestamp'].groupby(tweet_df['timestamp'].dayofweek).count())\n",
    "#tweet_df['hod'] = [t.hour for t in tweet_df['timestamp']]\n",
    "#tweet_df['dow'] = [t.dayofweek for t in tweet_df['timestamp']]\n",
    "#print(tweet_df.groupby(tweet_df['timestamp'].map(lambda x: x.day)).count())\n",
    "#df.set_index('timestamp').groupby(pd.TimeGrouper('D')).count().dropna()['text']\n",
    "\n",
    "    ###### figure 3\n",
    "    fig3 = plt.figure(figsize = (20, 10))\n",
    "    #date = data.index.date.astype('O')\n",
    "    x = dataD.index.tolist()\n",
    "    height = NTD\n",
    "    width = 0.8\n",
    "    plt.tight_layout() #ajustement des etiquttes\n",
    "    plt.bar(x, height, width, color='b')\n",
    "    plt.ylim(0,40000)\n",
    "    plt.ylabel('number of tweets')\n",
    "    plt.title('Distribution of tweets by day !')\n",
    "    plt.show()\n",
    "    \n",
    "    #####figure 4\n",
    "    fig4 = plt.figure(figsize = (20, 10))\n",
    "    \n",
    "    #date = data.index.date.astype('O')\n",
    "    x = dataH.index.tolist()\n",
    "    height = NTH\n",
    "    width = 0.01\n",
    "    plt.tight_layout() #ajustement des etiquttes\n",
    "    \n",
    "    plt.bar(x, height, width, color='g')\n",
    "    plt.ylim(0,2000)\n",
    "    \n",
    "    plt.ylabel('number of tweets')\n",
    "    plt.title('Distribution of tweets  by hour !')\n",
    "    plt.show()\n",
    "    print()\n",
    "     #date = data.index.date.astype('O')\n",
    "    #####figure 5  \n",
    "    fig5 = plt.figure(figsize = (20, 10))\n",
    "    #xlayout=[]\n",
    "    #for i in range(len(dataH24.index.tolist())) :\n",
    "    #    t=dataH24.index.tolist()[i]\n",
    "    #    xlayout.append(str(t[0])+'h'+str(t[1])+'min')\n",
    "    ##print(x)\n",
    "    x= dataH24.index.tolist()\n",
    "    #np.array(range(1,len(dataH24)))\n",
    "    height = NTH24\n",
    "    width = 0.8\n",
    "    plt.tight_layout() #ajustement des etiquttes\n",
    "    \n",
    "    plt.bar(x, height, width, color='g')\n",
    "    plt.ylim(0,10000)\n",
    "    \n",
    "    plt.ylabel('number of tweets')\n",
    "    plt.title('Distribution of tweets  for each hour !')\n",
    "    plt.show()\n",
    "    \n",
    "    #####figure 6\n",
    "    fig6 = plt.figure(figsize = (20, 10))\n",
    "    \n",
    "    #date = data.index.date.astype('O')\n",
    "    x = dataH.index.tolist()\n",
    "    height = SNTH\n",
    "    width = 0.01\n",
    "    plt.tight_layout() #ajustement des etiquttes\n",
    "    \n",
    "    plt.bar(x, height, width, color='g')\n",
    "    plt.ylim(0,150000)\n",
    "    \n",
    "    plt.ylabel('number of tweets')\n",
    "    plt.title('Cumulative distribution of tweets  by hour !')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hastag anf mentions functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_hashtag(tweet_df):\n",
    "    tweet_df['hashtag'] = tweet_df['hashtag'].str.replace('#', ' ')\n",
    "    tweet_df['hashtag'] = tweet_df['hashtag'].str.lower()\n",
    "    remove_ponctuation = lambda x: ''.join(ch for ch in x if ch not in set(string.punctuation))\n",
    "    tweet_df['hashtag']=tweet_df['hashtag'].map(remove_ponctuation)\n",
    "    #nltk.word_tokenize(tweet_df['hashtag'][28])\n",
    "    #create a tab with all the # for frequence analysis\n",
    "    hashtag=[]\n",
    "    for text in tweet_df['hashtag']:\n",
    "        T=nltk.word_tokenize(text)\n",
    "        \n",
    "        for j in range(len(T)):\n",
    "            #print(T[j].lower())\n",
    "            hashtag.append(T[j].lower())\n",
    "    return (hashtag)\n",
    "\n",
    "def most_common_hastag(tab):\n",
    "    x=Counter(tab)\n",
    "    H=x.most_common()\n",
    "    h=[]\n",
    "    b=[]\n",
    "    for i in range(100):\n",
    "        #print(h)\n",
    "        h.append(H[i][1])\n",
    "        b.append(H[i][0])\n",
    "    data={'hashtag':h}\n",
    "    df=pd.DataFrame(index=b,data=data)\n",
    "    df.to_excel('mention_frequency_fr.xlsx')\n",
    "    fig_h = plt.figure(figsize = (20, 7))\n",
    "    #date = data.index.date.astype('O')\n",
    "    print(H[:250])\n",
    "    print(sum(h))\n",
    "    print(len(tab))\n",
    "    \n",
    "    x = range(len(b[:100]))\n",
    "    height = h[:100]\n",
    "    labels=b\n",
    "    width = 0.2\n",
    "    plt.tight_layout() #ajustement des etiquttes\n",
    "    plt.bar(x, height, width, color='b')\n",
    "    plt.xticks(x, labels, rotation='vertical')\n",
    "    plt.ylabel('hastag frequency')\n",
    "    plt.title('hastag frequency!')\n",
    "    plt.show()\n",
    "    \n",
    "#####################\n",
    "#  We create a function that enable to count the number of occurance of a hastag in a serie given\n",
    "\n",
    "def counthashtag(serie,hashtagR):\n",
    "    find_hashtag=lambda x: x.find(hashtagR)\n",
    "    c=0\n",
    "    for k in range(len(serie)):\n",
    "        text=serie.iloc[k].lower()\n",
    "        if find_hashtag(text)>=1:\n",
    "            c+=1\n",
    "    return(c)\n",
    "\n",
    "#######################\n",
    "def mention_tab(tweet_df):######## Mentions\n",
    "    remove_ponctuation = lambda x: ''.join(ch for ch in x if ch not in set(string.punctuation))\n",
    "    tweet_df['mention']=tweet_df['mention'].str.replace('@', ' ')\n",
    "    tweet_df['mention']=tweet_df['mention'].map(remove_ponctuation)\n",
    "\n",
    "    mention=[]\n",
    "    users=0\n",
    "    for i in range(len(tweet_df['mention'])):\n",
    "        if len(tweet_df['mention'].iloc[i])>0:\n",
    "            users+=1\n",
    "            g=nltk.word_tokenize(tweet_df['mention'].iloc[i])\n",
    "            for j in range(len(g)):\n",
    "                mention.append(g[j].lower())\n",
    "    return(mention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# terms analisys function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_word(df_tweet,langue,filename):    \n",
    "    tweet_mot=[]\n",
    "    #stop=stopwords.words(langue)+[\"\"\"’\"\"\"]+['…']+['a']\n",
    "    sw = stop_words.get_stop_words(langue)+[\"\"\"’\"\"\"]+['…']+['a']+['les']+['ça']+['cest']+['jai']+['â€¦']\n",
    "    file = open(filename,'w',encoding='utf-8')\n",
    "    for i in range(len(df_tweet['text'])):\n",
    "        text=nltk.word_tokenize(df_tweet['text'].iloc[i])\n",
    "        for j in range(len(text)):\n",
    "            text[j]=text[j].lower()#everithing in lower case\n",
    "            if text[j] not in sw:\n",
    "                tweet_mot.append(text[j])\n",
    "                file.write(text[j])\n",
    "                file.write('\\n')\n",
    "                \n",
    "    file.close()\n",
    "    ### tweet_mot is a list of all the words in the tweets sans les mot dans les stop word\n",
    "    ####\n",
    "    return(tweet_mot)\n",
    "\n",
    "def most_common_word(word_list,k,filename) :\n",
    "    #unigram\n",
    "    U=[]\n",
    "    W=[]\n",
    "    freq=Counter(word_list)\n",
    "    uni=freq.most_common(k)\n",
    "    for i in range(k):\n",
    "        #print(h)\n",
    "        U.append(uni[i][1])\n",
    "        W.append(uni[i][0])\n",
    "    data={'freq':U,'term':W}\n",
    "    df_U=pd.DataFrame(data=data)\n",
    "    print(uni)\n",
    "    #bigram\n",
    "    B=[]\n",
    "    WB=[]\n",
    "    tweet_bigram = bigrams(word_list)\n",
    "    freq_bigram= Counter(tweet_bigram)\n",
    "    bi=freq_bigram.most_common(k)\n",
    "    \n",
    "    for i in range(k):\n",
    "        #print(h)\n",
    "        B.append(bi[i][1])\n",
    "        WB.append(str(bi[i][0][0])+' '+str(bi[i][0][1]))\n",
    "    dataB={'term':WB,'freq':B}\n",
    "    df_B=pd.DataFrame(data=dataB)\n",
    "    dict_b = dict(zip(WB, B))\n",
    "    \n",
    "    print(bi)\n",
    "    \n",
    "    #trigram\n",
    "    tweet_trigram = trigrams(word_list)\n",
    "    freq_trigram= Counter(tweet_trigram)\n",
    "    tri=freq_trigram.most_common(k)\n",
    "    \n",
    "    T=[]\n",
    "    WT=[]\n",
    "    for i in range(k):\n",
    "        #print(h)\n",
    "        T.append(tri[i][1])\n",
    "        WT.append(str(tri[i][0][0])+' '+str(tri[i][0][1])+' '+str(tri[i][0][2]))\n",
    "    dataT={'term':WT,'freq':T}\n",
    "    df_T=pd.DataFrame(data=dataT)\n",
    "    dict_t = dict(zip(WT, T))\n",
    "    print(tri)\n",
    "    \n",
    "    \n",
    "    \n",
    "    writer = pd.ExcelWriter(filename)\n",
    "    df_U.to_excel(writer,'unigram')\n",
    "    df_B.to_excel(writer,'bigram')\n",
    "    df_T.to_excel(writer,'trigram')\n",
    "    writer.save()\n",
    "    #return(dict(freq),dict_b,dict_t) #use this to plot wordcloud\n",
    "    return(df_U,df_B,df_T)#return this one for return dataframe for the sentiment analisys\n",
    "###################################################################################################\n",
    "from wordcloud import WordCloud\n",
    "def plot_wc(filename,filesave):\n",
    "    text = open(filename,encoding=\"utf8\").read()\n",
    "    \n",
    "    # Generate a word cloud image\n",
    "    wordcloud = WordCloud().generate(text)\n",
    "    \n",
    "    # Display the generated image:\n",
    "    # the matplotlib way:\n",
    "    \n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # lower max_font_size\n",
    "    wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(filesave)\n",
    "    plt.show()\n",
    "\n",
    "def plot_wc_freq(filesave,freq):\n",
    "    #text = open(filename,encoding=\"utf8\").read()\n",
    "    \n",
    "    # Generate a word cloud image\n",
    "    wordcloud = WordCloud().fit_words(frequencies=freq)\n",
    "    \n",
    "    # Display the generated image:\n",
    "    # the matplotlib way:\n",
    "    \n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # lower max_font_size\n",
    "    wordcloud = WordCloud(max_font_size=40).fit_words(frequencies=freq)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(filesave)\n",
    "    plt.show()\n",
    "#######################################\n",
    "\n",
    "def co_matrix(df_tweet,langue):\n",
    "    com = defaultdict(lambda : defaultdict(int))\n",
    "    sw = stop_words.get_stop_words(langue)+[\"\"\"’\"\"\"]+['…']+['a']+['les']+['ça']+['cest']+['jai']+['â€¦']\n",
    "    # extracting each tweet\n",
    "    for i in range(len(df_tweet['text'])):\n",
    "        text=nltk.word_tokenize(df_tweet['text'].iloc[i])\n",
    "        terms_only = [term for term in text if term not in sw]\n",
    "     \n",
    "        # Build co-occurrence matrix\n",
    "        for k in range(len(terms_only)-1):            \n",
    "            for j in range(k+1, len(terms_only)):\n",
    "                w1, w2 = sorted([terms_only[k], terms_only[j]])                \n",
    "                if w1 != w2:\n",
    "                    com[w1][w2] += 1\n",
    "    com_max = []\n",
    "    # For each term, look for the most common co-occurrent terms\n",
    "    for t1 in com:\n",
    "        t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "        for t2, t2_count in t1_max_terms:\n",
    "            com_max.append(((t1, t2), t2_count))\n",
    "    # Get the most frequent co-occurrences\n",
    "    terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "    ##############\n",
    "    df_com=pd.DataFrame()\n",
    "    df_com[\"term\"] = np.nan\n",
    "    df_com[\"freq\"] = np.nan\n",
    "    for i in range(150):\n",
    "        df_com.loc[i]=pd.Series({\"term\":str(terms_max[i][0][0])+' '+str(terms_max[i][0][1]),\"freq\":terms_max[i][1] })\n",
    "        \n",
    "    #df_com.to_excel('com_term_en.xlsx')\n",
    "    print(terms_max[:20])\n",
    "    return(terms_max,df_com)\n",
    "\n",
    "#####################################\n",
    "def score(text,lang):\n",
    "    labMT,labMTvector,labMTwordList = emotionFileReader(stopval=0.0,lang=lang,returnVector=True)\n",
    "    words = text.split()\n",
    "    s=0\n",
    "    k=0\n",
    "    for word in words :\n",
    "        if word in labMTwordList:\n",
    "            s+= float(labMT[word][1])\n",
    "            k+=1 #number of word that labMT count\n",
    "    if k==0:\n",
    "        return(0)\n",
    "    return(s/k)\n",
    "#####################################\n",
    "def sentiment_evaluation(df,lang): # THe entry is a data frame composed of 2 column : 'term' and 'freq'  \n",
    "    #df['score']=np.nan\n",
    "    S=[]\n",
    "    for i in range(len(df)):\n",
    "        S.append(score(df['term'].iloc[i],lang))\n",
    "    df['score']=S\n",
    "    df_final=df[df['score']>=1]# we drop all the 0\n",
    "    df_final['med']=df['score'].median()\n",
    "    df_final['mean']=df['score'].mean()\n",
    "    df_final['std']=df['score'].std()\n",
    "    df_final['freq /1000']=df['freq']/df['freq'].sum()*1000\n",
    "    df_final.to_excel('score_tab_tri_fr.xlsx')\n",
    "    return(df_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### create n  text file with one tweet eact time\n",
    "def tweet_to_text(df,folder):# de la forme '\\folder_name\n",
    "    newpath = r'C:\\Users\\Robin'+folder\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    for i in range(len(df['text'])):\n",
    "        \n",
    "        filename=folder[1:]+'/tweet'+str(i)+\"_\"+str(df['id'].iloc[i])+'.txt'\n",
    "        file = open(filename,'w',encoding='utf-8')\n",
    "        #for j in range(len(doc_stories_clean[i])):\n",
    "        text=df['text'].iloc[i]\n",
    "        file.write(text)\n",
    "        file.write('\\n')\n",
    "        file.close()\n",
    "##################\"\n",
    "\n",
    "def create_df_user(df):\n",
    "    df1=df.groupby(df['users'])\n",
    "    N=[]\n",
    "    I=[]\n",
    "    gb=df1.groups\n",
    "    for i,item in enumerate(gb) :\n",
    "        N.append(item)\n",
    "        I.append(i)\n",
    "    d = {'id_str': N}\n",
    "    df_user=pd.DataFrame(index=I,data=d)\n",
    "    df_user[\"user_id\"] = np.nan\n",
    "    df_user[\"screen_name\"]= np.nan\n",
    "    df_user[\"name\"]= np.nan\n",
    "    df_user[\"user_description\"] = np.nan\n",
    "    df_user[\"statuses_count\"] = np.nan\n",
    "    df_user[\"followers_count\"] = np.nan\n",
    "    df_user[\"friends_count\"] = np.nan\n",
    "    df_user[\"langue\"] = np.nan\n",
    "    df_user[\"created_at\"] = np.nan\n",
    "    \n",
    "    return(df_user)\n",
    "############################ create the datframe for each user of the campaign with key information\n",
    "def user_info(df_user):\n",
    "    e=0\n",
    "    k=0\n",
    "    start_time = time.time()\n",
    "    print(start_time)\n",
    "    limit=api.rate_limit_status('users').get('resources').get('users').get('/users/show/:id').get('remaining')\n",
    "    limit2=api2.rate_limit_status('users').get('resources').get('users').get('/users/show/:id').get('remaining')\n",
    "    for i in range(len(df_user)):\n",
    "        if limit>1 :\n",
    "            try:\n",
    "                user_info=api.get_user(screen_name=df_user['id_str'].iloc[i])\n",
    "                \n",
    "                k+=1\n",
    "                limit-=1\n",
    "                print('limit1=',limit)\n",
    "                df_user.loc[i] = pd.Series({\"id_str\":df_user['id_str'].iloc[i],\"user_id\":user_info.id, \n",
    "                                            \"screen_name\":user_info.screen_name, \"name\":user_info.name,\"user_description\":user_info.description,\n",
    "                                            \"statuses_count\":user_info.statuses_count,\"followers_count\":user_info.followers_count,\n",
    "                                            \"friends_count\":user_info.friends_count, \"langue\":user_info.lang,\n",
    "                                            \"created_at\": user_info.created_at})\n",
    "            \n",
    "        \n",
    "            except:\n",
    "                e+=1\n",
    "                limit-=1\n",
    "                print(e,'erreur pour',k,'succes')\n",
    "        \n",
    "        elif limit2>1 :\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))    \n",
    "            try:\n",
    "                user_info=api2.get_user(screen_name=df_user['id_str'].iloc[i])\n",
    "                k+=1\n",
    "                limit2-=1\n",
    "                print('limit2=',limit2)\n",
    "                df_user.loc[i] = pd.Series({\"id_str\":df_user['id_str'].iloc[i],\"user_id\":user_info.id, \n",
    "                                            \"screen_name\":user_info.screen_name, \"name\":user_info.name,\"user_description\":user_info.description,\n",
    "                                            \"statuses_count\":user_info.statuses_count,\"followers_count\":user_info.followers_count,\n",
    "                                            \"friends_count\":user_info.friends_count, \"langue\":user_info.lang,\n",
    "                                            \"created_at\": user_info.created_at})\n",
    "            \n",
    "        \n",
    "            except:\n",
    "                e+=1\n",
    "                limit2-=1    \n",
    "                print(e,'erreur pour',k,'succes')\n",
    "        \n",
    "        else:\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            sleep(15*61-400)\n",
    "            limit=api.rate_limit_status('users').get('resources').get('users').get('/users/show/:id').get('remaining')\n",
    "    return(df_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second cleaning fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nombre_mot(df):\n",
    "    nb_mots=[]\n",
    "    for i in range(len(df['text'])):\n",
    "        text=nltk.word_tokenize(df['text'].iloc[i])\n",
    "        nb_mots.append(len(text))\n",
    "    df['nb_mots']=nb_mots\n",
    "#####################################################################################################################\n",
    "def distribution(df,column,titre,xlim,ylim): #trace la distribution du colone de dataframe\n",
    "    N=[]\n",
    "    I=[]\n",
    "    for i in range(xlim):\n",
    "        I.append(i)\n",
    "        N.append(len(df[df[column]==i]))\n",
    "    fig1 = plt.figure(figsize = (30, 10))\n",
    "    x = I\n",
    "    height = N\n",
    "    width = 0.5\n",
    "    plt.tight_layout() #ajustement des etiquttes\n",
    "    \n",
    "     #fisrt graph\n",
    "    plt.bar(x, height, width, color='b')\n",
    "    plt.ylim(0,ylim)\n",
    "    plt.xlim(0,xlim)\n",
    "    plt.title(titre)\n",
    "    plt.show()\n",
    "\n",
    "    #############################\n",
    "    \n",
    "def drop_by_nb_of_word(df,nb_mots):\n",
    "    d=0\n",
    "    e=0\n",
    "    i=0\n",
    "    while i <len((df['text'])):\n",
    "        if df['nb_mots'].iloc[i]<nb_mots:\n",
    "            try:\n",
    "                df.drop(df.index[i],inplace=True)\n",
    "                d+=1\n",
    "            except:\n",
    "                e+=1\n",
    "        i+=1\n",
    "    print(d,\"tweets have been deleted by nb of word\", '\\n', 'there are',e,'errors')\n",
    "    return(d)\n",
    "#############################\n",
    "\n",
    "def drop_by_user(df,users):\n",
    "    d=0\n",
    "    e=0\n",
    "    i=0\n",
    "    while i <len((df['user'])):\n",
    "        if df['user'].iloc[i] in users:\n",
    "            try:\n",
    "                df.drop(df.index[i],inplace=True)\n",
    "                d+=1\n",
    "            except:\n",
    "                e+=1\n",
    "        i+=1\n",
    "    print(d,\"tweets have been deleted by users\", '\\n', 'there are',e,'errors')\n",
    "    return(d)\n",
    "#############################\n",
    "\n",
    "remove_ponctuation = lambda x: ''.join(ch for ch in x if ch not in set(string.punctuation))\n",
    "lower= lambda x: x.lower()\n",
    "def formalize(df):\n",
    "    df['user']=df['user'].map(lower)\n",
    "    df['mention']=df['mention'].map(lower)\n",
    "    df['text']=df['text'].map(lower)\n",
    "    df['hashtag']=df['hashtag'].map(lower)\n",
    "    df['mention']=df['mention'].str.replace('@', '')\n",
    "    df['mention']=df['mention'].map(remove_ponctuation)\n",
    "#############################\n",
    "\n",
    "def drop_by_mention(df,mention):\n",
    "    d=0\n",
    "    e=0\n",
    "    i=0\n",
    "    while i <len((df['text'])):\n",
    "        if str(df['mention'].iloc[i]) in mention :\n",
    "            try:\n",
    "                df.drop(df.index[i],inplace=True)\n",
    "                d+=1\n",
    "            except:\n",
    "                e+=1\n",
    "        i+=1\n",
    "    print(d,\"tweets have been deleted by mention\", '\\n', 'there are',e,'errors')\n",
    "    return(d)\n",
    "#############################    \n",
    "def drop_by_hashtag(df,find_hashtag):\n",
    "    d=0\n",
    "    e=0\n",
    "    i=0\n",
    "    while i <len((df['text'])):\n",
    "        text=str(df['hashtag'].iloc[i])\n",
    "        if len(find_hashtag.findall(text))>=int(1):\n",
    "            try:\n",
    "                df.drop(df.index[i],inplace=True)\n",
    "                d+=1\n",
    "            except:\n",
    "                e+=1\n",
    "        i+=1\n",
    "    print(d,\"tweets have been deleted by hashtag\", '\\n', 'there are',e,'errors')\n",
    "    return(d)\n",
    "#############################\n",
    "def drop_by_word(df,find_word):\n",
    "    d=0\n",
    "    e=0\n",
    "    i=0\n",
    "    while i <len((df['text'])):\n",
    "        text=lower(df['text'].iloc[i])\n",
    "        if len(find_word.findall(text))>=int(1):\n",
    "            try:\n",
    "                df.drop(df.index[i],inplace=True)\n",
    "                d+=1\n",
    "            except:\n",
    "                e+=1\n",
    "        i+=1\n",
    "    print(d,\"tweets have been deleted by word\", '\\n', 'there are',e,'errors')\n",
    "    return(d)\n",
    "#############################\n",
    "\n",
    "def save(df,filename):\n",
    "    df.to_json(filename)\n",
    "    #df2=pd.read_json(filename, orient=\"columns\")\n",
    "\n",
    "#############################\n",
    "def cleaning2(df,users,mentions,find_hashtag,find_word):\n",
    "    df= df.reset_index(drop=True)#réinitialise the index\n",
    "    add_nombre_mot(df)\n",
    "    nb_mots=list(df['nb_mots'].quantile([0.1]))[0]\n",
    "    print(distribution(df,'nb_mots','number of word',75,4000))\n",
    "    df_final= df[df['nb_mots']>=5]\n",
    "    print(len(df)-len(df_final),'tweets have been deleted by nb of word')\n",
    "    print(distribution(df_final,'nb_mots','number of word',75,4000))\n",
    "    formalize(df_final)\n",
    "    drop_by_user(df_final,users)\n",
    "    print(distribution(df_final,'nb_mots','number of word',75,4000))\n",
    "    drop_by_mention(df_final,mentions)\n",
    "    print(distribution(df_final,'nb_mots','number of word',75,4000))\n",
    "    drop_by_hashtag(df_final,find_hashtag)\n",
    "    print(distribution(df_final,'nb_mots','number of word',75,4000))\n",
    "    drop_by_word(df_final,find_word)\n",
    "    print(distribution(df_final,'nb_mots','number of word',75,4000))\n",
    "       \n",
    "    return(df_final)    \n",
    "###############################\n",
    "def category(df,find_Sexual_assault,find_rape,find_Sexual_harrasment):\n",
    "    d=0\n",
    "    e=0\n",
    "    i=0\n",
    "    category=[]\n",
    "    while i <len((df['text'])):\n",
    "        text=str(df['text'].iloc[i])\n",
    "        if len(find_Sexual_harrasment.findall(text))>=int(1):\n",
    "            try:\n",
    "                category.append('harrasment')\n",
    "                d+=1\n",
    "            except:\n",
    "                category.append(0)\n",
    "                e+=1\n",
    "        elif  len(find_Sexual_assault.findall(text))>=int(1):\n",
    "            try:\n",
    "                category.append('assault')\n",
    "                d+=1\n",
    "            except:\n",
    "                category.append(0)\n",
    "                e+=1\n",
    "        \n",
    "        elif len(find_rape.findall(text))>=int(1):\n",
    "            try:\n",
    "                category.append('rape')\n",
    "                d+=1\n",
    "            except:\n",
    "                category.append(0)\n",
    "                e+=1\n",
    "        else :\n",
    "            category.append(0)\n",
    "        i+=1\n",
    "    df['category']=category\n",
    "    df_harrasment=df[df['category']=='harrasment']\n",
    "    df_assault=df[df['category']=='assault']\n",
    "    df_rape=df[df['category']=='rape']\n",
    "    print('The category harrasment', 'is now composed of :',len(df_harrasment['text']),\"tweets\", '\\n',\n",
    "          'The category assault', 'is now composed of :',len(df_assault['text']),\"tweets\", '\\n'\n",
    "          'The category rape', 'is now composed of :',len(df_rape['text']),\"tweets\", '\\n'\n",
    "          'there are',e,'errors')\n",
    "    return(df_rape,df_assault,df_harrasment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
